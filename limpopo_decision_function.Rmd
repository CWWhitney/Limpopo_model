---
title: "Generating and calculating causal models for Limpopo"
author: "Cory Whitney, Eike Luedeling"
output: 
 html_document:
      toc: true 
      toc_float: true
      toc_collapsed: false
bibliography:
 - packages.bib
 - references.bib 
---


```{r, include = F}
#set global options for knitr chunks 
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  fig.width=5, 
  fig.height=3.5
)

# devtools::install_github("eikeluedeling/decisionSupport")
library(decisionSupport)
library(dplyr)
library(patchwork)
library(tidyverse)
library(Evapotranspiration)

```

```{r, warning = F, include = F}

#Automatically write R package citation entries to a .bib file
knitr::write_bib(c(.packages(),
                   'dplyr',
                   'patchwork',
                   'plyr',
                   'tidyverse',
                   'ggplot2', 
                   'decisionSupport',
                   'nasapower',
                   'Evapotranspiration'), 'packages.bib')

```

## The model

Decision-makers often wish to have a quantitative basis for their decisions. However,‘hard data’ is often missing or unattainable for many important variables, which can paralyze the decision-making processes or lead decision-makers to conclude that large research efforts are needed before a decision can be made. That is, many variables decision makers must consider cannot be precisely quantified, at least not without unreasonable effort. The major objective of (prescriptive) decision analysis is to support decision-making processes faced with this problem. Following the principles of Decision Analysis can allow us to make forecasts of decision outcomes without precise numbers, as long as probability distributions describing the possible values for all variables can be estimated. 

The `decisionSupport` package implements this as a Monte Carlo simulation, which generates a large number of plausible system outcomes, based on random numbers for each input variable that are drawn from user-specified probability distributions. This approach is useful for determining whether a clearly preferable course of action can be delineated based on the present state of knowledge without the need for further information. If the distribution of predicted system outcomes does not imply a clearly preferable decision option, variables identified as carrying decision-relevant uncertainty can then be targeted by decision-supporting research.

The `mcSimulation` function from the `decisionSupport` package can be applied to conduct decision analysis [@R-decisionSupport]. The function requires three inputs:

1. an `estimate` of the joint probability distribution of the input variables. These specify the names and probability distributions for all variables used in the decision model. These distributions aim to represent the full range of possible values for each component of the model. 
1. a `model_function` that predicts decision outcomes based on the variables named in a separate data table. This R function is customized by the user to address a particular decision problem to provide the decision analysis model.  
1. `numberOfModelRuns`	indicating the number of times to run the model function.

These inputs are provided as arguments to the `mcSimulation` function, which conducts a Monte Carlo analysis with repeated model runs based on probability distributions for all uncertain variables. The data table and model are customized to fit the particulars of a specific decision.

### The `estimate`

To support the model building process we design an input table to store the `estimate` values. The table is stored locally as `limpopo_input_table.csv` and contains many of the basic values for the analysis. This table contains all the input variables used in the model. Their distributions are described by 90% confidence intervals, which are specified by lower (5% quantile) and upper (95% quantile) bounds, as well as the shape of the distribution. This model uses four different distributions:

1.	`const` – a constant value
1.	`norm` – a normal distribution
1.	`tnorm_0_1` – a truncated normal distribution that can only have values between 0 and 1 (useful for probabilities; note that 0 and 1, as well as numbers outside this interval are not permitted as inputs)
1.	`posnorm` – a normal distribution truncated at 0 (only positive values allowed)

For a full list of possible distributions, type `?random.estimate1d ` in your R console. When specifying confidence intervals for truncated distributions, note that approximately 5% of the random values should ‘fit’ within the truncation interval on either side. If there is not enough space, the function will generate a warning (usually it will still work, but the inputs may not look like you intended them to).

### The `model_function`

![Model of the social effects of altered river flows on the sustainability of livelihoods in the Limpopo Basin ](figures/Collective_Model.png)

The decision model is coded as an R function which takes in the variables provided in the data table and generates a model output, such as the Net Present Value. 

In the following we use of various decisionSupport functions, which use the `tidyverse` libraries [@tidyverse2019] including `ggplot2` [@R-ggplot2], `plyr` [@R-plyr] and `dplyr` [@R-dplyr] among others in the [R programming language](https://www.r-project.org/) [@R-base]. 

Here we generate a model as a function using `decisionSupport` library we use the `decisionSupport` functions `vv()` to produce time series with variation from a pre-defined mean and coefficient of variation, `chance_event()` to simulate whether events occur and `discount()` to discount values along a time series adn generate a Net Present Value for our intervention comparison.


```{r make_variables, include=FALSE}
input_table<-"data/limpopo_input_table.csv"

make_variables <- function(est,n=1){ x <- random(rho=est, n=n)
for(i in colnames(x)) assign(i, as.numeric(x[1,i]),envir=.GlobalEnv)}

# run the function on the input_table
make_variables(estimate_read_csv(input_table))
```




```{r model}
limpopo_decision_function <- function(x, varnames){


# generating boundary conditions for the simulation run 

# how much rainwater is available
# for now we used data from some random climate diagram on the internet
rainfall<-sapply(1:12,function(x) eval(parse(text=paste0("prec_",x))))

effective_rainfall<-sapply(rainfall,function(x) min(x,effprec_high))
effective_rainfall<-sapply(effective_rainfall,function(x) max(x,effprec_low))


# we'll compute crop water needs based on ET0 (computed based on the Hargreaves
# Samani equation, as implemented in the Evapotranspiration package). Input
# temperature data comes from the NASAPOWER dataset (accessed through the 
# nasapower package)
# The data will be based on scenarios that represent conditions during real
# years in the past
# To get from ET0 to crop water use, we need to multiply ET0 with a crop
# coefficient (kc), which is estimated for each month

ET0<-sapply(1:12,function(x) eval(parse(text=paste0("ET0_",x)))) # in mm

kc<-sapply(1:12,function(x) eval(parse(text=paste0("kc_",x)))) # in mm

cropwat_need<-ET0*kc # in mm

irrigation_need<-cropwat_need-effective_rainfall # in mm


# define river flow and eflow for each month ####
# Base river flow data from 1920 to 2010, Letaba River at EWR site EWR4 (Letaba Ranch upstream Little Letaba confluence)
pre_livestock_river_flow<-sapply(1:12,function(x) eval(parse(text=paste0("river_flow_",x)))) # in m3 / month
eflow<-sapply(1:12,function(x) eval(parse(text=paste0("eflow_",x)))) # in m3 / month

# watering livestock
# assuming that this is more or less stable throughout the year, but varies a bit
livestock_water_needs<-vv(livestock_water_need,var_CV,12)

# assuming that the eflows aren't affecting ability to water livestock and that there's always enough
# water for all the livestock
river_flow<-pre_livestock_river_flow-livestock_water_needs

# calculating the farmed area

demand_for_farm_area<-n_subsistence_farmers*necessary_farm_size_per_household

farmed_area<-min(available_area, demand_for_farm_area)*(1-unused_sociopolit)


total_cropwater_need<-cropwat_need*farmed_area*10 # total water need in m3 (the 10 is the mm to m3/ha conversion)
total_effective_rainfall<-effective_rainfall*farmed_area*10 # total effective rainfall

# total irrigation need
total_irrigation_need<-total_cropwater_need-total_effective_rainfall # in m3

# water losses are calculated from the efficiency of the pumps and the water allocation
efficiency_pumps<-vv(effi_pump,var_CV,12)
efficiency_irrig_scheduling<-vv(effi_sched,var_CV,12)
efficiency_pumps<-sapply(efficiency_pumps, function(x)  min(x,1))
efficiency_pumps<-sapply(efficiency_pumps, function(x)  max(x,0))
efficiency_irrig_scheduling<-sapply(efficiency_irrig_scheduling, function(x)  min(x,1))
efficiency_irrig_scheduling<-sapply(efficiency_irrig_scheduling, function(x)  max(x,0))

water_losses_share<-(1-efficiency_pumps*efficiency_irrig_scheduling)

irrigation_water_need<-total_irrigation_need/(1-water_losses_share)

usable_river_flow<-sapply(1:12,function(x) max(0,river_flow[x]-minimum_flow_to_operate_pumps))

eflow_usable_river_flow<-sapply(1:12,function(x) max(0,river_flow[x]-max(eflow[x],minimum_flow_to_operate_pumps)))

# how much water gets extracted from the river

extracted_river_water<-sapply(1:12, function(x) min(usable_river_flow[x],irrigation_water_need[x]))
eflow_extracted_river_water<-sapply(1:12, function(x) min(eflow_usable_river_flow[x],irrigation_water_need[x]))


# calculate damage to crop production due to lack of irrigation water
water_shortfall<- (irrigation_water_need-extracted_river_water)
eflow_water_shortfall<- (irrigation_water_need-eflow_extracted_river_water)  

irrigation_shortfall<-water_shortfall*(1-water_losses_share)
eflow_irrigation_shortfall<-eflow_water_shortfall*(1-water_losses_share)

crop_water_gap<-irrigation_shortfall/(cropwat_need*farmed_area*10)
eflow_crop_water_gap<-eflow_irrigation_shortfall/(cropwat_need*farmed_area*10)


# calculate how much water is left after farmers extracted water
river_flow_downstream<-river_flow-extracted_river_water
eflow_river_flow_downstream<-river_flow-eflow_extracted_river_water

## what happens if we can't meet the water needs?


return(list(Downstream_river_flow=river_flow_downstream,
            Downstream_river_flow_eflow=eflow_river_flow_downstream,
            Downstream_difference=eflow_river_flow_downstream-river_flow_downstream,
            Crop_water_gap=crop_water_gap,
            Crop_water_gap_eflow=eflow_crop_water_gap,
            Crop_water_gap_difference=eflow_crop_water_gap-crop_water_gap,
            Mean_Crop_water_gap_difference=mean(eflow_crop_water_gap-crop_water_gap)))
  
}

```

#### Perform a Monte Carlo simulation with scenarios 

Using the model function above, we can perform a Monte Carlo simulation with the `mcSimulation()` function from `decisionSupport`. This function generates distributions of all variables in the input table as well as the specified model outputs (see `return()` function above) by calculating random draws in our defined `limpopo_decision_function()`. Make sure that all the variables in the input table are included in the model (erroneous variables listed there can cause issues with some of the post-hoc analyses). 

The `numberOfModelRuns` argument is an integer indicating the number of model runs for the Monte Carlo simulation. Unless the model function is very complex, 10,000 runs is a reasonable choice (for complex models, 10,000 model runs can take a while, so especially when the model is still under development, it often makes sense to use a lower number).

We'll first have to make a scenario file, for which we can use data for 1980 to 2020

```{r}
library(Evapotranspiration)
data("constants")

library(nasapower)

ag_d <- get_power(
  community = "ag",
  lonlat = c(31.08,-23.7),
  pars = c("T2M_MAX", "T2M_MIN", "PRECTOTCORR"),
  dates = c("1981-01-01", "2020-12-31"),
  temporal_api = "daily"
)

years<-1981:2009

colnames(ag_d)[c(3:5, 8, 9, 10)] <-
  c("Year", "Month", "Day", "Tmax", "Tmin", "Precipitation")

Inputs <- ReadInputs(c("Tmin", "Tmax"), ag_d, stopmissing = c(50, 50, 50))

ET <-
  ET.HargreavesSamani(
    Inputs,
    constants,
    ts = "daily",
    message = "yes",
    AdditionalStats = "yes",
    save.csv = "no"
  )

ETdata <- data.frame(year = years)
ETdata[, month.abb[1:12]] <- NA
for (yyyy in years)
  ETdata[which(ETdata$year == yyyy), 2:13] <-
  ET$ET.Monthly[as.character(yyyy + 0:11 / 12)]

rain <-
  aggregate(ag_d$Precipitation,
            by = list(ag_d$Year, ag_d$Month),
            FUN = sum)
raindata <- data.frame(year = years)
raindata[, month.abb[1:12]] <- NA
for (yyyy in years)
  raindata[which(raindata[, 1] == yyyy), 2:13] <-
  rain[which(rain[, 1] == yyyy), 3]


scenario_variables <-
  c(paste0("river_flow_", 1:12),
    paste0("ET0_", 1:12),
    paste0("prec_", 1:12),
    paste0("eflow_", 1:12))

Scenarios <- data.frame(Variable = scenario_variables, param = "both")

eflows<-read.csv("data/Letaba_eflows_exceedence_m3_per_s.csv",fileEncoding="UTF-8-BOM")
eflowsort <-
  eflows[, c(1, order(unlist(sapply(colnames(eflows)[2:13], function(x)
    which(month.abb[1:12] == x)))) + 1)]
eflow_exceedance<-eflowsort[which(eflowsort$Exceedence == 80),]
eflow_per_month<-eflow_exceedance[2:13]*c(31,28,31,30,31,30,31,31,30,31,30,31)*3600*24


present_flows<-read.csv("data/Letaba_modelled_present_flows_m3_per_s.csv",fileEncoding="UTF-8-BOM")
presentflowsort <-
  present_flows[, c(1, order(unlist(sapply(colnames(present_flows)[2:13], function(x)
    which(month.abb[1:12] == x)))) + 1)]
presentflow_permonth<-data.frame(cbind(presentflowsort[,1],t(t(presentflowsort[,2:13])*c(31,28,31,30,31,30,31,31,30,31,30,31)*3600*24)))
colnames(presentflow_permonth)[1]<-"Year"

for (yyyy in years)
{
  Scenarios[, paste0("y_", yyyy)] <- NA
  for (mm in 1:12)
  {
    Scenarios[which(Scenarios$Variable == paste0("ET0_", mm)), paste0("y_", yyyy)] <-
      ETdata[which(ETdata$year == yyyy), 1 + mm]
    Scenarios[which(Scenarios$Variable == paste0("prec_", mm)), paste0("y_", yyyy)] <-
      raindata[which(raindata$year == yyyy), 1 + mm]
    Scenarios[which(Scenarios$Variable == paste0("river_flow_", mm)), paste0("y_", yyyy)] <-
      presentflow_permonth[which(presentflow_permonth$Year == yyyy), 1 + mm]
    Scenarios[which(Scenarios$Variable == paste0("eflow_", mm)), paste0("y_", yyyy)] <-
      eflow_per_month[mm]
  }
}

#natural flows not used so far
natural_flows<-read.csv("data/Letaba_modelled_natural_flows_m3_per_s.csv",fileEncoding="UTF-8-BOM")





write.csv(Scenarios, "data/scenarios_1980_2020.csv", row.names = FALSE)



```


```{r mcSimulation_scenarios}
mcSimulation_results <-
  scenario_mc(
    base_estimate = decisionSupport::estimate_read_csv("data/limpopo_input_table.csv"),
    scenarios = read.csv("data/scenarios_1980_2020.csv", fileEncoding =
                           "UTF-8-BOM"),
    model_function = limpopo_decision_function,
    numberOfModelRuns = 2e2,
    #run 1,000 times
    functionSyntax = "plainNames"
  )


```

#### Plot Net Present Value (NPV) distributions 

We can use the `plot_distributions()` function to produce one of the several plotting options for distribution outputs. This shows us an overlay of the full results of the Monte Carlo model of the decision options, i.e. the expected NPV if we choose to do the intervention `Interv_NPV` or not do the intervention `NO_Interv_NPV`.

```{r}
decisionSupport::plot_distributions(mcSimulation_object = mcSimulation_results,
                                    vars = c("Mean_Crop_water_gap_difference"),
                                    method = 'smooth_simple_overlay',
                                    base_size = 7)

```

#### Flow analysis

Here we plot the distribution of downstream flow difference over the entire simulated year. For this we use the `plot_cashflow()` function, which uses the `Downstream_difference` output from the `mcSimulation()` function to show the difference in downstream river flow over time.

```{r plot_cashflow_downstream}
plot_cashflow(mcSimulation_object = mcSimulation_results, 
              cashflow_var_name = "Downstream_difference",
              y_axis_name = "Downstream difference (m3)",
              x_axis_name = "Month")
```

Here a similar plot of `Crop_water_gap_difference` to show the crop water gap over time.

```{r plot_cashflow_downstream_cropwater_gap}
plot_cashflow(mcSimulation_object = mcSimulation_results, 
              cashflow_var_name = "Crop_water_gap_difference",
              y_axis_name = "Crop water gap difference (share crop need)",
              x_axis_name = "Month")
```

#### Projection to Latent Structures (PLS) analysis

Projection to Latent Structures (PLS), also sometimes known as Partial Least Squares regression is a multivariate statistical technique that can deal with multiple colinear dependent and independent variables [@wold_pls-regression_2001]. It can be used as another means to assess the outcomes of a Monte Carlo model. Read more in ['A Simple Explanation of Partial Least Squares' by Kee Siong Ng](http://users.cecs.anu.edu.au/~kee/pls.pdf).

Variable Importance in Projection (VIP) scores estimate the importance of each variable in the projection used in a PLS mode. VIP is a parameter used for calculating the cumulative measure of the influence of individual $X$-variables on the model. For a given PLS dimension, $a$, the squared PLS weight $(Wa)2$ of that term is multiplied by the explained sum of squares ($SS$) of that $PLS$ dimension; and the value obtained is then divided by the total explained $SS$ by the PLS model and multiplied by the number of terms in the model. The final $VIP$ is the square root of that number.

$VIP_{PLS} = K\times (\frac{[\sum_{a=1}^{A}(W_{a}^{2} \times SSY_{comp,a})]}{SSY_{cum}})$

The VIP is a weighted combination overall components of the squared PLS weights ($Wa$), where $SSY_{comp,a}$ is the sum of squares of $Y$ explained by component $a$, $A$ is the total number of components, and $K$ is the total number of variables. The average VIP is equal to 1 because the $SS$ of all VIP values is equal to the number of variables in $X$. A variable with a VIP Score close to or greater than 1 (one) can be considered important. The input is a PLS model and the output is a set of column vectors equal in length to the number of variables included in the model. See @galindo-prieto_variable_2014 for a detailed description of variations of VIP analysis.

We apply a post-hoc analysis to the `mcSimulation()` outputs with `plsr.mcSimulation()` to determine the *Variable Importance in the Projection (VIP)* score and coefficients of a *Projection to Latent Structures (PLS)* regression model. This function uses the outputs of the `mcSimulation()` selecting all the input variables from the decision analysis function in the parameter `object` and then runs a PLS regression with an outcome variable defined in the parameter `resultName`. We use the code `names(mcSimulation_results$y)[73]` to select the outcome variable `r names(mcSimulation_results$y)[73]`, which is an element of the list `y` in our `mcSimulation_results` outputs (this must be a character element). 

```{r}
mcSimulation_pls<-mcSimulation_results
mcSimulation_pls$x<-mcSimulation_pls$x[, !names(mcSimulation_pls$x) == "Scenario"]

pls_result <- plsr.mcSimulation(object = mcSimulation_pls,
                  resultName = names(mcSimulation_results$y)[73], ncomp = 1)

```

We run the `plot_pls()` on the results from `plsr.mcSimulation()` with a number of standard settings. The length of the bars is equal to VIP with a vertical line at '1' on the x-axis indicating a standard cut-off for VIP used for variable selection. The overall plot only shows those variables with a VIP > 0.8, which is the common threshold for variable selection [@lanzanova_improving_2019, @luedeling_decision-focused_2016]. The colors of the bars represent the positive or negative coefficient of the given input variable with the output variable.

Here we import the input table again to replace the labels for the variables on the y-axis. The input table can include a `label` and `variable` column. The standard labels (from the `variable` column) are usually computer readable and not very nice for a plot. The `plot_pls()` function uses the text in the `label` column as replacement for the default text in the `variable` column.  

```{r}
input_table <- read.csv("data/limpopo_input_table.csv")

plot_pls(pls_result, input_table = input_table, threshold = 0.5)

```


### Addendum

The objective of the procedures used in the `decisionSupport` package is to make it easier for analysts to produce decision-relevant information that adequately reflects the imperfect nature of the information we usually have. Adding  probabilistic elements to a simulation adds substantial value to an analysis. Mostly, it avoids making spurious assumptions, replacing uncertainty with ‘best bets’ and producing results that do not reflect the knowledge limitations that make decision-making so challenging. More information on all this is contained in the [decisionSupport manual](https://cran.r-project.org/web/packages/decisionSupport/decisionSupport.pdf), especially under `welfareDecisionAnalysis`.

### References

